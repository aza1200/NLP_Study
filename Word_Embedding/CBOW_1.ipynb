{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고 사이트 : https://www.kaggle.com/code/alincijov/nlp-starter-continuous-bag-of-words-cbow\n",
    "# 참고 사이트 : https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314\n",
    "\n",
    "# word embedding : 단어를 유의미한 수치의 벡터로 만들기 위한방법\n",
    "# one hot encoding 등 여러방법이 있다 ...\n",
    "\n",
    "# one hot vector 단점\n",
    "# 1. 단어수 늘어남에 따라 차원수 늘어남\n",
    "# 2. application r과 밀접하게 결합되어 // 다른 모델로 전이학습\n",
    "# 혹은 단어의 추가/삭제가 용이하지 않다.\n",
    "# 3. embedding 의 목적은 단어의 문맥적 의미를 파악하는 것이나, \n",
    "# 단어와 단어사이의 문맥적 관계를 파악할 방법이 없다. \n",
    "\n",
    "# CBOW : 중심단어 기준으로 중간단어 예측하는거임\n",
    "\n",
    "# CBOW architecture\n",
    "# - the word embeddings as inputs (idx)\n",
    "# - the linear model as the hidden layer\n",
    "# - the log_softmax as the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW\n",
    "# 문맥으로 부터 중간단어예측\n",
    "# 인풋단어들에 대해서 same target words 다.\n",
    "# skip-gram 보다 빠름, 빈번한 단어들에 대해서는 조금더 정확함.\n",
    "\n",
    "# Skip-gram\n",
    "# 단어로부터 문맥예측\n",
    "# fake task 는 이 네트워크의 input과 output 에 관심이 있는것이 \n",
    "# 아니라, hidden layer 의 weight 와 word vector 에만 관심이있다.\n",
    "# input 단어의 접근성은 상관없다 거리 상관 x \n",
    "# 작은 양에 대해서 훈련시 훈련잘됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# remove special characters and replace with ' '\n",
    "# [] => 나열된 문자 혹은 범위에 해당하는 문자\n",
    "\n",
    "# 특수문자 제거\n",
    "sentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)\n",
    "\n",
    "# remove 1 letter word 첫부분 한단어,끝부분 한단어, 중간 한단어짜리 지움\n",
    "# ?: non-capturing group -> 대충 매칭되도 무시하겠다 이런뜻\n",
    "sentences =  re.sub(r'(?:^| )\\w(?:$| )', ' ', sentences).strip()\n",
    "\n",
    "# lower all characters\n",
    "sentences = sentences.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary 변환\n",
    "words = sentences.split()\n",
    "vocab = set(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 10\n",
    "\n",
    "# sliding window 의 한쪽 면이라고 생각하는게 편한거 같다. \n",
    "context_size = 2\n",
    "print(f\"vocab size : {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation\n",
    "word_to_idx = {word: i for i,word in enumerate(vocab)}\n",
    "idx_to_word = {i:word for i,word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words -> 정제된 문장\n",
    "# voca -> 단어 중복제거 \n",
    "# embde_dim -> 차원 변환 결과\n",
    " \n",
    " \n",
    "# data - [(context), target]\n",
    "\n",
    "data = []\n",
    "for i in range(2, len(words)-2):\n",
    "    context = [words[i-2], words[i-1], words[i+1], words[i+2]]\n",
    "    target = words[i]\n",
    "    data.append((context, target))\n",
    "\n",
    "print(f\"length of data : {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서 부터 pytorch 로 바꿔보면 괜찮을듯 싶다. \n",
    "\n",
    "# Embeddings \n",
    "embeddings =  np.random.random_sample((vocab_size, embed_dim))\n",
    "print(f\"embedding shape : {embeddings.shape}\")\n",
    "# 43개의 input 10개의 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행렬곱 시키는거\n",
    "def linear(m, theta):\n",
    "    w = theta\n",
    "    return m.dot(w)\n",
    "\n",
    "# 결과값 softmax 시키는 것\n",
    "def log_softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return np.log(e_x / e_x.sum())\n",
    "\n",
    "# logs => softmax log 값 취한거\n",
    "def NLLLoss(logs, targets):\n",
    "    out = logs[range(len(targets)), targets]\n",
    "    return -out.sum()/len(out)\n",
    "\n",
    "# \n",
    "def log_softmax_crossentropy_with_logits(logits,target):\n",
    "    \n",
    "    out = np.zeros_like(logits)\n",
    "    out[np.arange(len(logits)),target] = 1\n",
    "    \n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    \n",
    "    return (- out + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫 벡터를 만드는 대신 숫자로 바로 만들어버려서 \n",
    "# import numpy as np\n",
    "# theta = np.random.uniform(-1, 1, (2 * context_size * embed_dim, vocab_size))\n",
    "# context_idxs = [1,2,3,4]\n",
    "\n",
    "# context_idxs.shape => (4,1) <- window 갯수 4개라서 그럼\n",
    "\n",
    "# @내 생각은 () @(10,43) \n",
    "# theta.shape => (40,43) <- \n",
    "# embeddings => (43,10)\n",
    "\n",
    "# \n",
    "def forward(context_idxs, theta):\n",
    "\n",
    "    m = embeddings[context_idxs].reshape(1, -1)\n",
    "    n = linear(m, theta)\n",
    "    o = log_softmax(n)\n",
    "\n",
    "    # o.shape => (1, 43)\n",
    "    return m, n, o\n",
    "# forward(context_idxs, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(preds, theta, target_idxs):\n",
    "    m, n, o = preds\n",
    "    # m => 1 번째 결과값\n",
    "    # n => 2 번쨰 결과값\n",
    "    # o => softmax 에다가 log 취한거\n",
    "    dlog = log_softmax_crossentropy_with_logits(n, target_idxs)\n",
    "    dw = m.T.dot(dlog)\n",
    "                \n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(theta, grad, lr=0.03):\n",
    "    theta -= grad * lr\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses = {}\n",
    "# 균등분포로 np.random.uniform 뽑아냄\n",
    "theta = np.random.uniform(-1, 1, (2 * context_size * embed_dim, vocab_size))\n",
    "for epoch in range(80):\n",
    "\n",
    "    losses =  []\n",
    "\n",
    "    for context, target in data:\n",
    "        context_idxs = np.array([word_to_idx[w] for w in context])\n",
    "        \n",
    "        preds = forward(context_idxs, theta)\n",
    "        \n",
    "        target_idxs = np.array([word_to_idx[target]])\n",
    "        loss = NLLLoss(preds[-1], target_idxs)\n",
    "        \n",
    "        losses.append(loss)\n",
    "\n",
    "        grad = backward(preds, theta, target_idxs)\n",
    "        theta = optimize(theta, grad, lr=0.03)\n",
    "        \n",
    "     \n",
    "    epoch_losses[epoch] = losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ix = np.arange(0,80)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Epoch/Losses', fontsize=20)\n",
    "plt.plot(ix,[epoch_losses[i][0] for i in ix])\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Losses', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(words):\n",
    "    context_idxs = np.array([word_to_idx[w] for w in words])\n",
    "    preds = forward(context_idxs, theta)\n",
    "    word = idx_to_word[np.argmax(preds[-1])]\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(['we', 'are', 'to', 'study'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy():\n",
    "    wrong = 0\n",
    "\n",
    "    for context, target in data:\n",
    "        if(predict(context) != target):\n",
    "            wrong += 1\n",
    "            \n",
    "    return (1 - (wrong / len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
